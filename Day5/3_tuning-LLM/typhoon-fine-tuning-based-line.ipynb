{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0a390da7",
      "metadata": {
        "id": "0a390da7",
        "papermill": {
          "duration": 0.149952,
          "end_time": "2024-04-20T04:52:11.850095",
          "exception": false,
          "start_time": "2024-04-20T04:52:11.700143",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Hello! and welcome to my Typhoon fine tuning notebook.\n",
        "\n",
        "\n",
        "Hello! and welcome to my Typhoon fine tuning notebook.\n",
        "\n",
        "Learn more about the model: https://arxiv.org/abs/2312.13951"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "257fa2cb",
      "metadata": {
        "id": "257fa2cb",
        "papermill": {
          "duration": 0.147534,
          "end_time": "2024-04-20T04:52:12.146158",
          "exception": false,
          "start_time": "2024-04-20T04:52:11.998624",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Install requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7720d692",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7720d692",
        "outputId": "9d76b59f-81d3-46d6-daaa-19cb406ca09e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Notebook cleaned.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import IPython\n",
        "import sys\n",
        "\n",
        "def clean_notebook():\n",
        "    IPython.display.clear_output(wait=True)\n",
        "    print(\"Notebook cleaned.\")\n",
        "\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "\n",
        "!pip install datasets peft accelerate bitsandbytes\n",
        "\n",
        "# Clean up the notebook\n",
        "clean_notebook()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21e6d936",
      "metadata": {
        "id": "21e6d936",
        "papermill": {
          "duration": 0.145914,
          "end_time": "2024-04-20T04:54:27.251167",
          "exception": false,
          "start_time": "2024-04-20T04:54:27.105253",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Set up environment variables\n",
        "\n",
        "This is set with kaggle secret collection. If you're runing with other enviroment, they can be set .env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "aaf0d781",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-20T04:54:27.549815Z",
          "iopub.status.busy": "2024-04-20T04:54:27.549468Z",
          "iopub.status.idle": "2024-04-20T04:54:27.953378Z",
          "shell.execute_reply": "2024-04-20T04:54:27.952586Z"
        },
        "id": "aaf0d781",
        "papermill": {
          "duration": 0.555595,
          "end_time": "2024-04-20T04:54:27.955815",
          "exception": false,
          "start_time": "2024-04-20T04:54:27.400220",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['HF_TOKEN']         =\"code\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91b7293f",
      "metadata": {
        "id": "91b7293f",
        "papermill": {
          "duration": 0.146046,
          "end_time": "2024-04-20T04:54:28.249126",
          "exception": false,
          "start_time": "2024-04-20T04:54:28.103080",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Select dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "31c74ef9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-20T04:54:28.549807Z",
          "iopub.status.busy": "2024-04-20T04:54:28.549463Z",
          "iopub.status.idle": "2024-04-20T04:54:28.553847Z",
          "shell.execute_reply": "2024-04-20T04:54:28.552937Z"
        },
        "id": "31c74ef9",
        "papermill": {
          "duration": 0.155525,
          "end_time": "2024-04-20T04:54:28.555905",
          "exception": false,
          "start_time": "2024-04-20T04:54:28.400380",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "dataset_id = \"Thaweewat/thai-med-pack\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c2ccb8d",
      "metadata": {
        "id": "3c2ccb8d",
        "papermill": {
          "duration": 0.147398,
          "end_time": "2024-04-20T04:54:28.849712",
          "exception": false,
          "start_time": "2024-04-20T04:54:28.702314",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Load tokenizer and model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a80d66d5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190,
          "referenced_widgets": [
            "7d245d13cec14fffbeabfdc6896b1dbf",
            "988ae91c763347c69b434d515b8a9f7e",
            "e3d6721a2af64f74a0aa9a9219352172",
            "14964aba0eca453b8bc95e00b80a8734",
            "06e5d937d1284c32a57ba82e66259d24",
            "63f8796692f7497d94227a9171fc9c27",
            "7d88052277a84fbc9193236321a72b8a",
            "ac877611509348f19b4ee27bbc2bf91e",
            "5b347a348b464123a5f52319aecbbebb",
            "0be9b6592f2e41c380a81cb14ef51f20",
            "59eea92369c74b00a8e6e7c7e621ad93"
          ]
        },
        "execution": {
          "iopub.execute_input": "2024-04-20T04:54:29.147136Z",
          "iopub.status.busy": "2024-04-20T04:54:29.146325Z",
          "iopub.status.idle": "2024-04-20T04:55:53.612048Z",
          "shell.execute_reply": "2024-04-20T04:55:53.611225Z"
        },
        "id": "a80d66d5",
        "outputId": "3626ce0c-f7ae-4a81-ba8f-350663dd66d6",
        "papermill": {
          "duration": 84.613319,
          "end_time": "2024-04-20T04:55:53.614426",
          "exception": false,
          "start_time": "2024-04-20T04:54:29.001107",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a832ceba39cf48aab0ec8f5083fbaac7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "\n",
        "model_id = \"scb10x/llama-3-typhoon-v1.5-8b\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b57dd36e",
      "metadata": {
        "id": "b57dd36e",
        "papermill": {
          "duration": 0.147415,
          "end_time": "2024-04-20T04:55:53.911491",
          "exception": false,
          "start_time": "2024-04-20T04:55:53.764076",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "Then we have to apply some preprocessing to the model to prepare it for training. For that use the `prepare_model_for_kbit_training` method from PEFT."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8228170",
      "metadata": {
        "id": "a8228170",
        "papermill": {
          "duration": 0.146126,
          "end_time": "2024-04-20T04:55:54.203547",
          "exception": false,
          "start_time": "2024-04-20T04:55:54.057421",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Set up trainable parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e79a1890",
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5ef07582",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 4235264 || all params: 4544835584 || trainable%: 0.09318849761936734\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=64,\n",
        "    target_modules=[\"lm_head\",\"all-linear\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "01bc977b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-20T04:55:54.498222Z",
          "iopub.status.busy": "2024-04-20T04:55:54.497381Z",
          "iopub.status.idle": "2024-04-20T04:55:55.607447Z",
          "shell.execute_reply": "2024-04-20T04:55:55.606644Z"
        },
        "id": "01bc977b",
        "papermill": {
          "duration": 1.259278,
          "end_time": "2024-04-20T04:55:55.609727",
          "exception": false,
          "start_time": "2024-04-20T04:55:54.350449",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cea8bf43",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-04-20T04:55:56.264531Z",
          "iopub.status.busy": "2024-04-20T04:55:56.263638Z",
          "iopub.status.idle": "2024-04-20T04:55:56.271999Z",
          "shell.execute_reply": "2024-04-20T04:55:56.271056Z"
        },
        "id": "cea8bf43",
        "outputId": "4c10f41b-dd87-49d4-c8cd-ada9e90e627c",
        "papermill": {
          "duration": 0.170612,
          "end_time": "2024-04-20T04:55:56.274308",
          "exception": false,
          "start_time": "2024-04-20T04:55:56.103696",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): LlamaForCausalLM(\n",
            "      (model): LlamaModel(\n",
            "        (embed_tokens): Embedding(128256, 4096)\n",
            "        (layers): ModuleList(\n",
            "          (0-31): 32 x LlamaDecoderLayer(\n",
            "            (self_attn): LlamaSdpaAttention(\n",
            "              (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "              (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "              (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "              (rotary_emb): LlamaRotaryEmbedding()\n",
            "            )\n",
            "            (mlp): LlamaMLP(\n",
            "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
            "              (act_fn): SiLU()\n",
            "            )\n",
            "            (input_layernorm): LlamaRMSNorm()\n",
            "            (post_attention_layernorm): LlamaRMSNorm()\n",
            "          )\n",
            "        )\n",
            "        (norm): LlamaRMSNorm()\n",
            "      )\n",
            "      (lm_head): lora.Linear(\n",
            "        (base_layer): Linear(in_features=4096, out_features=128256, bias=False)\n",
            "        (lora_dropout): ModuleDict(\n",
            "          (default): Dropout(p=0.05, inplace=False)\n",
            "        )\n",
            "        (lora_A): ModuleDict(\n",
            "          (default): Linear(in_features=4096, out_features=32, bias=False)\n",
            "        )\n",
            "        (lora_B): ModuleDict(\n",
            "          (default): Linear(in_features=32, out_features=128256, bias=False)\n",
            "        )\n",
            "        (lora_embedding_A): ParameterDict()\n",
            "        (lora_embedding_B): ParameterDict()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfd3bd4d",
      "metadata": {
        "id": "dfd3bd4d",
        "papermill": {
          "duration": 0.149958,
          "end_time": "2024-04-20T04:56:01.430032",
          "exception": false,
          "start_time": "2024-04-20T04:56:01.280074",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Preprocess dataset\n",
        "\n",
        "I parsed into `# Instruction: # Input: # Response:` also I added `<answer></answer>` xml and response but you may modify it. I just like this way :D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ea9a05e6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-20T04:56:01.736850Z",
          "iopub.status.busy": "2024-04-20T04:56:01.736444Z",
          "iopub.status.idle": "2024-04-20T04:58:05.336735Z",
          "shell.execute_reply": "2024-04-20T04:58:05.335720Z"
        },
        "id": "ea9a05e6",
        "papermill": {
          "duration": 123.758599,
          "end_time": "2024-04-20T04:58:05.339372",
          "exception": false,
          "start_time": "2024-04-20T04:56:01.580773",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset(dataset_id)\n",
        "\n",
        "\n",
        "def parse(text: str) -> str:\n",
        "    try:\n",
        "        question_search = re.search(r'\\[INST\\](.*)\\[/INST\\]', text, re.IGNORECASE)\n",
        "        question = question_search.group(1).strip()\n",
        "\n",
        "\n",
        "        answer_search = re.search(r'\\[/INST\\](.*)\\</s\\>', text, re.IGNORECASE)\n",
        "        answer = answer_search.group(1).strip()\n",
        "        spec = f\"\"\"<s>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "### Instruction:\n",
        "Act as a doctor and response the input question from a patient in Thai language with XML format <answer>Your answer</answer>\n",
        "### Input:\n",
        "{question}\n",
        "### Response:\n",
        "<answer> {answer} </answer>\n",
        "</s>\"\"\"\n",
        "    except:\n",
        "        print(text)\n",
        "        raise\n",
        "    return {\n",
        "        \"spec\": spec\n",
        "    }\n",
        "\n",
        "\n",
        "# parse(data[\"train\"][\"text\"][0])\n",
        "# data.map(lambda )\n",
        "data = data.map(lambda samples: parse(samples[\"text\"]), batch_size=8,num_proc=os.cpu_count())\n",
        "data = data.map(lambda samples: tokenizer(samples[\"spec\"]), batched=True, batch_size=8,num_proc=os.cpu_count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "LMOy3m9iKGIt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMOy3m9iKGIt",
        "outputId": "748b5592-755e-4b3e-bb63-7b1bc1a0e15a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'spec', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 189190\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "bpghMyNXKVP1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        },
        "id": "bpghMyNXKVP1",
        "outputId": "eb95eacd-615e-4e3d-b65a-76dc11b6d9d0"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**text:**\n",
              "\n",
              "<s>[INST] สวัสดีค่ะ ช่วงเวลาประมาณ 3 สัปดาห์ หนูรู้สึกท้อแท้ สิ้นหวังกับทุกสิ่งทุกอย่าง ถูกกดดันอยู่ตลอดเวลา ทั้งที่บ้านและโรงเรียน หนูไม่เคยบอกความรู้สึกกับใคร เหมือนอยู่ตัวคนเดียว รู้สึกกลัวกับทุกอย่าง เวลาอยู่คนเดียวแล้วมันรู้สึกเหงา คิดเรื่องต่างๆนาๆ ร้องให้ออกมาทุกครั้ง มีวันหนึ่งหนูทนไม่ไหวเลยเล่าให้เพื่อนคนหนึ่งฟังและร้องให้ออกมาหนักมาก แต่มันก็ไม่ดีขึ้นเลย รู้สึกเหนื่อยตลอดเวลา หนูลองทำแบบทดสอบแล้วได้ 14 อาการแบบนี้เข้าข่ายโรคซึมเศร้าไหมค่ะ#รบกวนด้วยน่ะค่ะ [/INST] สวัสดีค่ะ จากที่เล่ามา คิดว่า มีภาวะทางอารมณ์ไม่ปกติ อาจมีภาวะเครียด หรืออาจมีภาวะโรคซึมเศร้าค่ะผู้ป่วยโรคซึมเศร้าจะมีอาการที่เปลี่ยนแปลงทั้งอารมณ์ ความรู้สึกนึกคิด ความจำ ความสัมพันธ์กับคนรอบข้างเปลี่ยนไป และอาการแสดงทางร่างกายต่างๆ โดยอาการอาจเปลี่ยนแปลงอย่างรวดเร็วเป็นสัปดาห์ หรือค่อยเป็นค่อยไปเป็นก็ได้-อาการทางอารมณ์ เช่น เศร้า หดหู่ อ่อนไหว บางคนอาจมีความรู้สึกไม่แจ่มใส เบื่อหน่าย บางคนมีอาการหงุดหงิด ฉุนเฉียวได้-ความรู้สึกนึกคิด เช่น มองอะไรแย่หมด ท้อแท้หมดหวัง รู้สึกไร้ค่า ไร้ความสามารถ บางคนอาจมีอารมณ์ชั่ววูบอยากทำร้ายตนเอง-สมาธิความจำแย่ลง-ความสัมพันธ์กับคนรอบข้างเปลี่ยนไป อาจจะเก็บตัว ไม่ค่อยพูด ทำงานแย่ลง-อาการแสดงทางร่างกาย เช่น อ่อนเพลีย ไม่มีแรง นอบหลับๆตื่นๆ ไม่เจริญอาหาร น้ำหนักลด ท้องผูก ท้องอืด ปวดหัว ปวดตามตัว*คนไข้หากมีอาการมากขึ้น อาจมีอาการหลงผิด ประสาทหลอน หูแว่วได้*โรคซึมเศร้านี้ยังมีลักษณะอาการคล้ายโรคอื่นอีก เช่น โรควิตกกังวล โรคอารมณ์แปรปรวน โรคสมองอักเสบ โรคไทรอยด์ทำงานบกพร่อง เป็นต้น-การรักษา: ในรายที่เป็นมาก แพทย์จะให้ยาช่วย แต่บางรายที่เป็นไม่มากแพทย์อาจจะพูดคุยให้คำแนะนำเพื่อให้อาการดีขึ้นดังนั้นจึงแนะนำให้ ควรเล่าถึงภาวะทางอารมณ์ให้คนใกล้ชิดหรือคนในครอบครัวฟังเพื่อให้อารมณ์ผ่อนคลายและช่วยแก้ไขปัญหานะคะและแนะนำว่าควรไปพบแพทย์เพื่อรับการวินิจฉัยและได้รับการรักษาอย่างเหมาะสมขอเป็นกำลังใจให้นะคะ </s>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "---"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**spec:**\n",
              "\n",
              "<s>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
              "### Instruction:\n",
              "Act as a doctor and response the input question from a patient in Thai language with XML format <answer>Your answer</answer>\n",
              "### Input:\n",
              "สวัสดีค่ะ ช่วงเวลาประมาณ 3 สัปดาห์ หนูรู้สึกท้อแท้ สิ้นหวังกับทุกสิ่งทุกอย่าง ถูกกดดันอยู่ตลอดเวลา ทั้งที่บ้านและโรงเรียน หนูไม่เคยบอกความรู้สึกกับใคร เหมือนอยู่ตัวคนเดียว รู้สึกกลัวกับทุกอย่าง เวลาอยู่คนเดียวแล้วมันรู้สึกเหงา คิดเรื่องต่างๆนาๆ ร้องให้ออกมาทุกครั้ง มีวันหนึ่งหนูทนไม่ไหวเลยเล่าให้เพื่อนคนหนึ่งฟังและร้องให้ออกมาหนักมาก แต่มันก็ไม่ดีขึ้นเลย รู้สึกเหนื่อยตลอดเวลา หนูลองทำแบบทดสอบแล้วได้ 14 อาการแบบนี้เข้าข่ายโรคซึมเศร้าไหมค่ะ#รบกวนด้วยน่ะค่ะ\n",
              "### Response:\n",
              "<answer> สวัสดีค่ะ จากที่เล่ามา คิดว่า มีภาวะทางอารมณ์ไม่ปกติ อาจมีภาวะเครียด หรืออาจมีภาวะโรคซึมเศร้าค่ะผู้ป่วยโรคซึมเศร้าจะมีอาการที่เปลี่ยนแปลงทั้งอารมณ์ ความรู้สึกนึกคิด ความจำ ความสัมพันธ์กับคนรอบข้างเปลี่ยนไป และอาการแสดงทางร่างกายต่างๆ โดยอาการอาจเปลี่ยนแปลงอย่างรวดเร็วเป็นสัปดาห์ หรือค่อยเป็นค่อยไปเป็นก็ได้-อาการทางอารมณ์ เช่น เศร้า หดหู่ อ่อนไหว บางคนอาจมีความรู้สึกไม่แจ่มใส เบื่อหน่าย บางคนมีอาการหงุดหงิด ฉุนเฉียวได้-ความรู้สึกนึกคิด เช่น มองอะไรแย่หมด ท้อแท้หมดหวัง รู้สึกไร้ค่า ไร้ความสามารถ บางคนอาจมีอารมณ์ชั่ววูบอยากทำร้ายตนเอง-สมาธิความจำแย่ลง-ความสัมพันธ์กับคนรอบข้างเปลี่ยนไป อาจจะเก็บตัว ไม่ค่อยพูด ทำงานแย่ลง-อาการแสดงทางร่างกาย เช่น อ่อนเพลีย ไม่มีแรง นอบหลับๆตื่นๆ ไม่เจริญอาหาร น้ำหนักลด ท้องผูก ท้องอืด ปวดหัว ปวดตามตัว*คนไข้หากมีอาการมากขึ้น อาจมีอาการหลงผิด ประสาทหลอน หูแว่วได้*โรคซึมเศร้านี้ยังมีลักษณะอาการคล้ายโรคอื่นอีก เช่น โรควิตกกังวล โรคอารมณ์แปรปรวน โรคสมองอักเสบ โรคไทรอยด์ทำงานบกพร่อง เป็นต้น-การรักษา: ในรายที่เป็นมาก แพทย์จะให้ยาช่วย แต่บางรายที่เป็นไม่มากแพทย์อาจจะพูดคุยให้คำแนะนำเพื่อให้อาการดีขึ้นดังนั้นจึงแนะนำให้ ควรเล่าถึงภาวะทางอารมณ์ให้คนใกล้ชิดหรือคนในครอบครัวฟังเพื่อให้อารมณ์ผ่อนคลายและช่วยแก้ไขปัญหานะคะและแนะนำว่าควรไปพบแพทย์เพื่อรับการวินิจฉัยและได้รับการรักษาอย่างเหมาะสมขอเป็นกำลังใจให้นะคะ </answer>\n",
              "</s>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "\n",
        "idx = 5\n",
        "\n",
        "display(Markdown(f\"**text:**\\n\\n{data['train'][idx]['text']}\"))\n",
        "display(Markdown(\"---\"))\n",
        "display(Markdown(f\"**spec:**\\n\\n{data['train'][idx]['spec']}\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d9c9c1e",
      "metadata": {
        "id": "2d9c9c1e",
        "papermill": {
          "duration": 0.148907,
          "end_time": "2024-04-20T04:58:05.639191",
          "exception": false,
          "start_time": "2024-04-20T04:58:05.490284",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed63e1cd",
      "metadata": {
        "id": "ed63e1cd",
        "papermill": {
          "duration": 0.151999,
          "end_time": "2024-04-20T04:58:05.945375",
          "exception": false,
          "start_time": "2024-04-20T04:58:05.793376",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "83901364",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "execution": {
          "iopub.execute_input": "2024-04-20T04:58:06.248805Z",
          "iopub.status.busy": "2024-04-20T04:58:06.248435Z",
          "iopub.status.idle": "2024-04-20T08:58:01.706047Z",
          "shell.execute_reply": "2024-04-20T08:58:01.705072Z"
        },
        "id": "83901364",
        "outputId": "414d5aac-4024-4ea5-e927-eb2e8d97ef85",
        "papermill": {
          "duration": 14395.760286,
          "end_time": "2024-04-20T08:58:01.858572",
          "exception": false,
          "start_time": "2024-04-20T04:58:06.098286",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/tuchsanai/anaconda3/envs/llm/lib/python3.12/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m model\u001b[38;5;241m.\u001b[39mgradient_checkpointing_enable()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/transformers/trainer.py:2118\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2118\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2121\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2122\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2123\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2124\u001b[0m ):\n\u001b[1;32m   2125\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2126\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
            "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/transformers/trainer.py:3045\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3043\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3044\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3045\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3047\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
            "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/accelerate/accelerator.py:2001\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1999\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2000\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2001\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "\n",
        "# Clear the GPU cache\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Set the padding token for the tokenizer\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Define the data collator\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "\n",
        "# Set up the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=9,  # Adjusted batch size\n",
        "    gradient_accumulation_steps=5,  # Adjust gradient accumulation steps\n",
        "    max_steps=35,\n",
        "    learning_rate=1e-5,\n",
        "    logging_steps=1,\n",
        "    output_dir=\"outputs\",\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    load_best_model_at_end=True,\n",
        "    save_strategy=\"no\",\n",
        "   \n",
        "    \n",
        "    \n",
        ")\n",
        "\n",
        "# Define the trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=data[\"train\"],\n",
        "    args=training_args,\n",
        "    data_collator=data_collator\n",
        "\n",
        ")\n",
        "\n",
        "# # Disable caching to silence warnings (enable for inference)\n",
        "model.config.use_cache = False\n",
        "\n",
        "# # Enable gradient checkpointing for memory efficiency\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c596c956",
      "metadata": {
        "id": "c596c956",
        "papermill": {
          "duration": 0.159903,
          "end_time": "2024-04-20T08:58:02.176515",
          "exception": false,
          "start_time": "2024-04-20T08:58:02.016612",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Export model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e10ab8eb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-20T08:58:02.495664Z",
          "iopub.status.busy": "2024-04-20T08:58:02.494042Z",
          "iopub.status.idle": "2024-04-20T08:58:04.522921Z",
          "shell.execute_reply": "2024-04-20T08:58:04.521783Z"
        },
        "id": "e10ab8eb",
        "papermill": {
          "duration": 2.190565,
          "end_time": "2024-04-20T08:58:04.525395",
          "exception": false,
          "start_time": "2024-04-20T08:58:02.334830",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/tuchsanai/anaconda3/envs/llm/lib/python3.12/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
          ]
        }
      ],
      "source": [
        "model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(\"outputs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18d72b99",
      "metadata": {
        "id": "18d72b99",
        "papermill": {
          "duration": 0.210631,
          "end_time": "2024-04-20T08:58:04.952003",
          "exception": false,
          "start_time": "2024-04-20T08:58:04.741372",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Test model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb6ab22b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-20T08:58:05.355010Z",
          "iopub.status.busy": "2024-04-20T08:58:05.354061Z",
          "iopub.status.idle": "2024-04-20T09:06:40.677057Z",
          "shell.execute_reply": "2024-04-20T09:06:40.676153Z"
        },
        "id": "fb6ab22b",
        "papermill": {
          "duration": 515.657837,
          "end_time": "2024-04-20T09:06:40.836233",
          "exception": false,
          "start_time": "2024-04-20T08:58:05.178396",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "/home/tuchsanai/anaconda3/envs/llm/lib/python3.12/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "### Instruction:\n",
            "Act as a doctor and response the input question from a patient in Thai language with XML format <answer>Your answer</answer>\n",
            "### Input:\n",
            "คือผมอยากทราบว่า อาการที่ผมเป็นตอนนี้คือกรดไหลย้อน หรือ เป็นสัญญาณของพิษสุนัขบ้าครับ ผมมีอาการ เเน่นๆ อึดอัดที่คอ เเล้วก็ กลืนน้ำลายลำบากครับ ก่อนหน้านี้มีไข้ต่ำ ปวดหัวนิดหน่อยครับ ช่วง 1 เดือนก่อน ผมทำงานเดินทางโดยจักรยานครับ ทางผ่านมีสุนัขอยู่ตามทางเยอะมากๆ (เเต่จากที่เห็นไม่ได้เห่าเเละไล่ตามผมครับ) เเล้วมาพึ่งมาเป็นอาการดังกล่าวช่วงนี้ครับ ผมจึงไม่เเน่ใจว่าเป็นกรดไหลย้อนหรือเป็นสัญญานอาการเเรกเริ่มของพิษสุนัขบ้าหรอครับ ผมอ่านเเล้วเห็นอาการคล้ายๆกันครับคุณ รบกวนด้วยนะครับ\n",
            "### Response:\n",
            "----------------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'<s>Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n### Instruction:\\nAct as a doctor and response the input question from a patient in Thai language with XML format <answer>Your answer</answer>\\n### Input:\\nคือผมอยากทราบว่า อาการที่ผมเป็นตอนนี้คือกรดไหลย้อน หรือ เป็นสัญญาณของพิษสุนัขบ้าครับ ผมมีอาการ เเน่นๆ อึดอัดที่คอ เเล้วก็ กลืนน้ำลายลำบากครับ ก่อนหน้านี้มีไข้ต่ำ ปวดหัวนิดหน่อยครับ ช่วง 1 เดือนก่อน ผมทำงานเดินทางโดยจักรยานครับ ทางผ่านมีสุนัขอยู่ตามทางเยอะมากๆ (เเต่จากที่เห็นไม่ได้เห่าเเละไล่ตามผมครับ) เเล้วมาพึ่งมาเป็นอาการดังกล่าวช่วงนี้ครับ ผมจึงไม่เเน่ใจว่าเป็นกรดไหลย้อนหรือเป็นสัญญานอาการเเรกเริ่มของพิษสุนัขบ้าหรอครับ ผมอ่านเเล้วเห็นอาการคล้ายๆกันครับคุณ รบกวนด้วยนะครับ\\n### Response:'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import  StoppingCriteria, StoppingCriteriaList\n",
        "\n",
        "class KeywordsStoppingCriteria(StoppingCriteria):\n",
        "    def __init__(self, keywords_ids:list):\n",
        "        self._i = 0\n",
        "        self.keywords = keywords_ids\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        text = tokenizer.decode(\n",
        "                input_ids[0],\n",
        "                skip_special_tokens=True\n",
        "        )\n",
        "        if text.strip().endswith(\"</answer>\"):\n",
        "            return True\n",
        "        if self._i % 50 == 0:\n",
        "            print(text)\n",
        "            print(\"-\" * 16)\n",
        "        self._i += 1\n",
        "        if input_ids[0][-1] in self.keywords:\n",
        "\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "stop_words = ['</answer>']\n",
        "\n",
        "\n",
        "stop_ids = [tokenizer.encode(w) for w in stop_words]\n",
        "stop_criteria = KeywordsStoppingCriteria(stop_ids)\n",
        "\n",
        "stopping_criteria = StoppingCriteriaList([stop_criteria])\n",
        "\n",
        "\n",
        "text = \"\"\"<s>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "### Instruction:\n",
        "Act as a doctor and response the input question from a patient in Thai language with XML format <answer>Your answer</answer>\n",
        "### Input:\n",
        "คือผมอยากทราบว่า อาการที่ผมเป็นตอนนี้คือกรดไหลย้อน หรือ เป็นสัญญาณของพิษสุนัขบ้าครับ ผมมีอาการ เเน่นๆ อึดอัดที่คอ เเล้วก็ กลืนน้ำลายลำบากครับ ก่อนหน้านี้มีไข้ต่ำ ปวดหัวนิดหน่อยครับ ช่วง 1 เดือนก่อน ผมทำงานเดินทางโดยจักรยานครับ ทางผ่านมีสุนัขอยู่ตามทางเยอะมากๆ (เเต่จากที่เห็นไม่ได้เห่าเเละไล่ตามผมครับ) เเล้วมาพึ่งมาเป็นอาการดังกล่าวช่วงนี้ครับ ผมจึงไม่เเน่ใจว่าเป็นกรดไหลย้อนหรือเป็นสัญญานอาการเเรกเริ่มของพิษสุนัขบ้าหรอครับ ผมอ่านเเล้วเห็นอาการคล้ายๆกันครับคุณ รบกวนด้วยนะครับ\n",
        "### Response:\"\"\"\n",
        "\n",
        "device = \"cuda:0\"\n",
        "\n",
        "lora_config = LoraConfig.from_pretrained('outputs')\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "outputs = model_to_save.generate(\n",
        "    **inputs, max_new_tokens=400,\n",
        "    stopping_criteria=stopping_criteria,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    no_repeat_ngram_size=10,\n",
        "    forced_eos_token_id=tokenizer.eos_token_id,\n",
        "    do_sample=True,\n",
        "    top_p=0.95\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "display(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c555090",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<s>Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n### Instruction:\\nAct as a doctor and response the input question from a patient in Thai language with XML format <answer>Your answer</answer>\\n### Input:\\nคือผมอยากทราบว่า อาการที่ผมเป็นตอนนี้คือกรดไหลย้อน หรือ เป็นสัญญาณของพิษสุนัขบ้าครับ ผมมีอาการ เเน่นๆ อึดอัดที่คอ เเล้วก็ กลืนน้ำลายลำบากครับ ก่อนหน้านี้มีไข้ต่ำ ปวดหัวนิดหน่อยครับ ช่วง 1 เดือนก่อน ผมทำงานเดินทางโดยจักรยานครับ ทางผ่านมีสุนัขอยู่ตามทางเยอะมากๆ (เเต่จากที่เห็นไม่ได้เห่าเเละไล่ตามผมครับ) เเล้วมาพึ่งมาเป็นอาการดังกล่าวช่วงนี้ครับ ผมจึงไม่เเน่ใจว่าเป็นกรดไหลย้อนหรือเป็นสัญญานอาการเเรกเริ่มของพิษสุนัขบ้าหรอครับ ผมอ่านเเล้วเห็นอาการคล้ายๆกันครับคุณ รบกวนด้วยนะครับ\\n### Response:'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92524db2",
      "metadata": {
        "id": "92524db2",
        "papermill": {
          "duration": 0.158763,
          "end_time": "2024-04-20T09:06:41.154360",
          "exception": false,
          "start_time": "2024-04-20T09:06:40.995597",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# (Optional) Push trained model to your Huggingface account"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "121a192f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-20T09:06:41.470406Z",
          "iopub.status.busy": "2024-04-20T09:06:41.469557Z",
          "iopub.status.idle": "2024-04-20T09:07:21.043389Z",
          "shell.execute_reply": "2024-04-20T09:07:21.042341Z"
        },
        "id": "121a192f",
        "papermill": {
          "duration": 39.733143,
          "end_time": "2024-04-20T09:07:21.045473",
          "exception": false,
          "start_time": "2024-04-20T09:06:41.312330",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "ename": "HfHubHTTPError",
          "evalue": "401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-665e2939-38941208543277a978679b3d;4de0941e-a605-4c56-a0ac-2128e327dea5)\n\nInvalid username or password.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_to_save\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtyphoon-med\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpush_to_hub(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtyphoon-med\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m model_to_save\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpush_to_hub(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtyphoon-med\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/transformers/utils/hub.py:873\u001b[0m, in \u001b[0;36mPushToHubMixin.push_to_hub\u001b[0;34m(self, repo_id, use_temp_dir, commit_message, private, token, max_shard_size, create_pr, safe_serialization, revision, commit_description, tags, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    870\u001b[0m repo_url \u001b[38;5;241m=\u001b[39m deprecated_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_url\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    871\u001b[0m organization \u001b[38;5;241m=\u001b[39m deprecated_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morganization\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 873\u001b[0m repo_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_repo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprivate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morganization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morganization\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# Create a new empty model card and eventually tag it\u001b[39;00m\n\u001b[1;32m    878\u001b[0m model_card \u001b[38;5;241m=\u001b[39m create_and_tag_model_card(\n\u001b[1;32m    879\u001b[0m     repo_id, tags, token\u001b[38;5;241m=\u001b[39mtoken, ignore_metadata_errors\u001b[38;5;241m=\u001b[39mignore_metadata_errors\n\u001b[1;32m    880\u001b[0m )\n",
            "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/transformers/utils/hub.py:689\u001b[0m, in \u001b[0;36mPushToHubMixin._create_repo\u001b[0;34m(self, repo_id, private, token, repo_url, organization)\u001b[0m\n\u001b[1;32m    686\u001b[0m             repo_id \u001b[38;5;241m=\u001b[39m repo_id\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    687\u001b[0m         repo_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00morganization\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 689\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprivate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m url\u001b[38;5;241m.\u001b[39mrepo_id\n",
            "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:119\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    117\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/huggingface_hub/hf_api.py:3365\u001b[0m, in \u001b[0;36mHfApi.create_repo\u001b[0;34m(self, repo_id, token, private, repo_type, exist_ok, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[1;32m   3362\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   3364\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3365\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3366\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   3367\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exist_ok \u001b[38;5;129;01mand\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m409\u001b[39m:\n\u001b[1;32m   3368\u001b[0m         \u001b[38;5;66;03m# Repo already exists and `exist_ok=True`\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.12/site-packages/huggingface_hub/utils/_errors.py:371\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 371\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-665e2939-38941208543277a978679b3d;4de0941e-a605-4c56-a0ac-2128e327dea5)\n\nInvalid username or password."
          ]
        }
      ],
      "source": [
        "model_to_save.push_to_hub(\"typhoon-med\")\n",
        "tokenizer.push_to_hub(\"typhoon-med\")\n",
        "model_to_save.config.push_to_hub(\"typhoon-med\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30683,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 15394.457684,
      "end_time": "2024-04-20T09:08:43.283811",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-04-20T04:52:08.826127",
      "version": "2.5.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06e5d937d1284c32a57ba82e66259d24": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0be9b6592f2e41c380a81cb14ef51f20": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14964aba0eca453b8bc95e00b80a8734": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0be9b6592f2e41c380a81cb14ef51f20",
            "placeholder": "​",
            "style": "IPY_MODEL_59eea92369c74b00a8e6e7c7e621ad93",
            "value": " 4/4 [00:10&lt;00:00,  2.32s/it]"
          }
        },
        "59eea92369c74b00a8e6e7c7e621ad93": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b347a348b464123a5f52319aecbbebb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "63f8796692f7497d94227a9171fc9c27": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d245d13cec14fffbeabfdc6896b1dbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_988ae91c763347c69b434d515b8a9f7e",
              "IPY_MODEL_e3d6721a2af64f74a0aa9a9219352172",
              "IPY_MODEL_14964aba0eca453b8bc95e00b80a8734"
            ],
            "layout": "IPY_MODEL_06e5d937d1284c32a57ba82e66259d24"
          }
        },
        "7d88052277a84fbc9193236321a72b8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "988ae91c763347c69b434d515b8a9f7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63f8796692f7497d94227a9171fc9c27",
            "placeholder": "​",
            "style": "IPY_MODEL_7d88052277a84fbc9193236321a72b8a",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "ac877611509348f19b4ee27bbc2bf91e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3d6721a2af64f74a0aa9a9219352172": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac877611509348f19b4ee27bbc2bf91e",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5b347a348b464123a5f52319aecbbebb",
            "value": 4
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
