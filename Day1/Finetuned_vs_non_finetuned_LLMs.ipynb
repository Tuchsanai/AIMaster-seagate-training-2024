{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![Logo - Colab.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "d6ZU5_bPMu3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare finetuned to non-finetuned Llama 2 LLMs, using [Lamini](https://www.lamini.ai/)\n",
        "\n",
        "- See the base Llama 2 struggle with various requests\n",
        "- See the finetuned Llama 2 for chat be able to handle requests much better\n",
        "- Learn prompt templates that finetuned Llama 2 uses, and do it super easily with `LlamaV2Runner` in Lamini.\n",
        "\n",
        "It's completely free to run."
      ],
      "metadata": {
        "id": "212CweEmMi5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 1: Authenticate with Google\n",
        "# @markdown Note: You will be asked to sign in with Google, connected to your Lamini account.\n",
        "\n",
        "from google.colab import auth\n",
        "import requests\n",
        "import os\n",
        "import yaml\n",
        "\n",
        "def authenticate_powerml():\n",
        "  auth.authenticate_user()\n",
        "  gcloud_token = !gcloud auth print-access-token\n",
        "  powerml_token_response = requests.get('https://api.powerml.co/v1/auth/verify_gcloud_token?token=' + gcloud_token[0])\n",
        "  print(powerml_token_response)\n",
        "  return powerml_token_response.json()['token']\n",
        "\n",
        "key = authenticate_powerml()\n",
        "\n",
        "config = {\n",
        "    \"production\": {\n",
        "        \"key\": key,\n",
        "        \"url\": \"https://api.powerml.co\"\n",
        "    }\n",
        "}\n",
        "\n",
        "keys_dir_path = '/root/.powerml'\n",
        "os.makedirs(keys_dir_path, exist_ok=True)\n",
        "\n",
        "keys_file_path = keys_dir_path + '/configure_llama.yaml'\n",
        "with open(keys_file_path, 'w') as f:\n",
        "  yaml.dump(config, f, default_flow_style=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3g1dl7e7F4U",
        "outputId": "a68118db-1613-4b24-bf8c-9b78653f5bbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Response [200]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 2: Install the open-source [Lamini library](https://pypi.org/project/lamini/) to use LLMs easily\n",
        "# @markdown Note: After installing, click the \"RESTART RUNTIME\" button at the end of the output, then go onto the next cell.\n",
        "# @markdown Lamini is just on a more recent version of numpy than Colab.\n",
        "!pip install --upgrade --force-reinstall --ignore-installed lamini"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iltj1wGW7I5k",
        "outputId": "042fe6e0-003c-4ff6-a372-3d9ec0ac544f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lamini\n",
            "  Downloading lamini-1.0.2-34-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic==1.10.* (from lamini)\n",
            "  Downloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lamini-configuration[yaml] (from lamini)\n",
            "  Downloading lamini_configuration-0.8.3-py3-none-any.whl (22 kB)\n",
            "Collecting requests (from lamini)\n",
            "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm (from lamini)\n",
            "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy (from lamini)\n",
            "  Downloading numpy-1.26.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy (from lamini)\n",
            "  Downloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonlines (from lamini)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Collecting pandas (from lamini)\n",
            "  Downloading pandas-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting faiss-cpu (from lamini)\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-storage-blob (from lamini)\n",
            "  Downloading azure_storage_blob-12.19.0-py3-none-any.whl (394 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn (from lamini)\n",
            "  Downloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.2.0 (from pydantic==1.10.*->lamini)\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Collecting azure-core<2.0.0,>=1.28.0 (from azure-storage-blob->lamini)\n",
            "  Downloading azure_core-1.29.5-py3-none-any.whl (192 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.4/192.4 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cryptography>=2.1.4 (from azure-storage-blob->lamini)\n",
            "  Downloading cryptography-41.0.5-cp37-abi3-manylinux_2_28_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting isodate>=0.6.1 (from azure-storage-blob->lamini)\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting attrs>=19.2.0 (from jsonlines->lamini)\n",
            "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyyaml<7.0,>=6.0 (from lamini-configuration[yaml]->lamini)\n",
            "  Downloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m705.5/705.5 kB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dateutil>=2.8.2 (from pandas->lamini)\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytz>=2020.1 (from pandas->lamini)\n",
            "  Downloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.5/502.5 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tzdata>=2022.1 (from pandas->lamini)\n",
            "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting charset-normalizer<4,>=2 (from requests->lamini)\n",
            "  Downloading charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.1/142.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna<4,>=2.5 (from requests->lamini)\n",
            "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<3,>=1.21.1 (from requests->lamini)\n",
            "  Downloading urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.6/104.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting certifi>=2017.4.17 (from requests->lamini)\n",
            "  Downloading certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.5/162.5 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting joblib>=1.1.1 (from scikit-learn->lamini)\n",
            "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0 (from scikit-learn->lamini)\n",
            "  Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
            "Collecting six>=1.11.0 (from azure-core<2.0.0,>=1.28.0->azure-storage-blob->lamini)\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting cffi>=1.12 (from cryptography>=2.1.4->azure-storage-blob->lamini)\n",
            "  Downloading cffi-1.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.9/443.9 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycparser (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob->lamini)\n",
            "  Downloading pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytz, faiss-cpu, urllib3, tzdata, typing-extensions, tqdm, threadpoolctl, six, pyyaml, pycparser, numpy, lamini-configuration, joblib, idna, charset-normalizer, certifi, attrs, scipy, requests, python-dateutil, pydantic, jsonlines, isodate, cffi, scikit-learn, pandas, cryptography, azure-core, azure-storage-blob, lamini\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.26.2 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.1.3 which is incompatible.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed attrs-23.1.0 azure-core-1.29.5 azure-storage-blob-12.19.0 certifi-2023.11.17 cffi-1.16.0 charset-normalizer-3.3.2 cryptography-41.0.5 faiss-cpu-1.7.4 idna-3.4 isodate-0.6.1 joblib-1.3.2 jsonlines-4.0.0 lamini-1.0.2 lamini-configuration-0.8.3 numpy-1.23.5 pandas-1.5.3 pycparser-2.21 pydantic-1.10.13 python-dateutil-2.8.2 pytz-2023.3.post1 pyyaml-6.0.1 requests-2.31.0 scikit-learn-1.2.2 scipy-1.11.3 six-1.16.0 threadpoolctl-3.2.0 tqdm-4.66.1 typing-extensions-4.5.0 tzdata-2023.3 urllib3-2.0.7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_cffi_backend",
                  "certifi",
                  "cffi",
                  "charset_normalizer",
                  "cryptography",
                  "dateutil",
                  "numpy",
                  "requests",
                  "six",
                  "yaml"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🚨 Note: After installing, click the \"RESTART RUNTIME\" button at the end of the output, then go onto the next cell.\n",
        "### Lamini is just on a more recent version of numpy than Colab.\n"
      ],
      "metadata": {
        "id": "WMGxBP-yQoCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare finetuned vs. non-finetuned LLMs"
      ],
      "metadata": {
        "id": "DXZYKJGK8a4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, you can import `BasicModelRunner` from the [Lamini library](https://lamini-ai.github.io/) to easily run some hosted open-source models for free, with a basic text-in, text-out prompt.\n",
        "\n",
        "This is so you can compare performance on a couple popular LLMs, running fast on GPUs."
      ],
      "metadata": {
        "id": "zj1XHrII8lAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BasicModelRunner(\"EleutherAI/pythia-70m\")\n",
        "model.load_data(\n",
        "            [\n",
        "                {\n",
        "                    \"input\": \"What kind of exercise is super for me?\",\n",
        "                    \"output\": \"Running\",\n",
        "                },\n",
        "                {\n",
        "                    \"input\": \"What kind of exercise is awesome for me?\",\n",
        "                    \"output\": \"Running\",\n",
        "                },\n",
        "            ]\n",
        ")\n",
        "model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxvMhQDrkb_V",
        "outputId": "4362009f-6460-4510-84e2-fc0f15f35b3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training job submitted! Check status of job 4158 here: https://app.lamini.ai/train/4158\n",
            "Finetuning process completed, model name is: 0b5abbbfa12cdad350574ab95ed90406ccbb5babe222ffe42a467c7d0f43f062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama import BasicModelRunner, QuestionAnswerModel, LlamaV2Runner"
      ],
      "metadata": {
        "id": "Phftiaie5aG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, you can load models from [HuggingFace](https://huggingface.co/), where open-source models are hosted.\n",
        "\n",
        "Llama 2 from Meta is all the rage!\n",
        "\n",
        "The `meta-llama/Llama-2-7b-hf` model is the base model that hasn't been finetuned yet. This means it's learned basic langauge from the internet and can auto-complete English sentences, but it hasn't learned that it should consistently follow instructions when you tell it to."
      ],
      "metadata": {
        "id": "x47C4CkK8tGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "non_finetuned = LlamaV2Runner(\"meta-llama/Llama-2-7b-hf\")"
      ],
      "metadata": {
        "id": "WLqoqmhR1Rto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can run the model on an input, like this:"
      ],
      "metadata": {
        "id": "PceV_o_b89mb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(non_finetuned(\"Tell me how to train my dog to sit\"))"
      ],
      "metadata": {
        "id": "OY9MhytZ2Wea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9db6fdb8-2aef-48af-9618-5e0887c4ca6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[INST] <<SYS>>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
            "\n",
            "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
            "<</SYS>>\n",
            "\n",
            "Tell me how to train my dog to sit [/INST]\n",
            "\n",
            "[INST] <<SYS>>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
            "\n",
            "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, that's a disaster."
      ],
      "metadata": {
        "id": "FvI_ys0Y9CcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(non_finetuned(\"What do you think of Mars?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBQLbiixl8UH",
        "outputId": "5df1af28-2a97-49da-8634-cebd23cabb0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[INST] <<SYS>>\n",
            "\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
            "\n",
            "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
            "<</SYS>>\n",
            "\n",
            "What do you think of Mars? [/INST]\n",
            "\n",
            "[INST] <<SYS>>\n",
            "\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
            "\n",
            "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try something closer to a Google search query.\n"
      ],
      "metadata": {
        "id": "NWXbaePj9FE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(non_finetuned(\"taylor swift's best friend\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIZJLMIjm7G1",
        "outputId": "14413551-0c45-43e6-c4cf-37f9c2c7a66e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[INST] <<SYS>>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
            "[/INST]\n",
            "[INST] <<SYS>> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
            "[/INST] [\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or, a customer service conversation."
      ],
      "metadata": {
        "id": "Zjpy6sTu9ITl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(non_finetuned(\"\"\"Agent: I'm here to help you with your Amazon deliver order.\n",
        "Customer: I didn't get my item\n",
        "Agent: I'm sorry to hear that. Which item was it?\n",
        "Customer: the blanket\n",
        "Agent:\"\"\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "bOZsr2gGr4ho",
        "outputId": "2b13e53a-24d8-4107-f912-a8c2250e8bfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-3943b9b9f8cf>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m print(non_finetuned(\"\"\"Agent: I'm here to help you with your Amazon deliver order.\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mCustomer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mI\u001b[0m \u001b[0mdidn\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mget\u001b[0m \u001b[0mmy\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mI\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mm\u001b[0m \u001b[0msorry\u001b[0m \u001b[0mto\u001b[0m \u001b[0mhear\u001b[0m \u001b[0mthat\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mWhich\u001b[0m \u001b[0mitem\u001b[0m \u001b[0mwas\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mCustomer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mblanket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m Agent:\"\"\"))\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama/runners/llama_v2_runner.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, system_prompt, output_type, multi_turn, turns, cue)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0minput_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLlamaV2Input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msystem_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         output_objects = self.llm(\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0moutput_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama/engine/typed_lamini.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"output_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama/engine/lamini.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input, output_type, stop_tokens, model_name, enable_peft, random, max_tokens, streaming)\u001b[0m\n\u001b[1;32m     84\u001b[0m         )\n\u001b[1;32m     85\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"completions\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_web_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;31m# https://lamini-ai.github.io/API/data/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama/engine/lamini.py\u001b[0m in \u001b[0;36mmake_web_request\u001b[0;34m(self, url, http_method, json)\u001b[0m\n\u001b[1;32m    260\u001b[0m         }\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_method\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mhttp_method\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"get\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \"\"\"\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    487\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    792\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1272\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compare, you can run this other Llama 2 model that has been finetuned!\n",
        "\n",
        "This one is about the same size, but a lot more usable from a chat perspective, when you provide the same input."
      ],
      "metadata": {
        "id": "VA9z2FG_9L0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_model = BasicModelRunner(\"meta-llama/Llama-2-7b-chat-hf\")\n",
        "print(finetuned_model(\"Tell me how to train my dog to sit \"))"
      ],
      "metadata": {
        "id": "yv4LzeDq3UEK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f1cd7cf-8a90-479f-b880-035fb4202d8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🐶\n",
            "\n",
            "Sure, I'd be happy to help you train your dog to sit! Here's a step-by-step guide on how to train your dog to sit:\n",
            "\n",
            "1. Choose a quiet and distraction-free area: Find a quiet area with no distractions where your dog can focus on you.\n",
            "2. Have treats ready: Choose your dog's favorite treats and have them ready to use as rewards.\n",
            "3. Stand in front of your dog: Stand in front of your dog and hold a treat close to their nose.\n",
            "4. Move the treat up and back: Slowly move the treat up and back, towards your dog's tail, while saying \"sit\" in a calm and clear voice.\n",
            "5. Dog will sit: As you move the treat, your dog will naturally sit down to follow the treat. The moment their bottom touches the ground, say \"good sit\" and give them the treat.\n",
            "6. Repeat the process: Repeat steps 3-5 several times, so your dog starts to associate the command \"sit\" with the action of sitting down.\n",
            "7. Gradually phase out the tre\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's still auto-completing the command.\n",
        "\n",
        "Llama 2 has been finetuned to take in instruction tags, e.g. between `[INST]` and `[/INST]`. You can use these, so it knows your instruction is complete."
      ],
      "metadata": {
        "id": "LLBPnqzA9RG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(finetuned_model(\"[INST]Tell me how to train your dog to sit[/INST]\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ka8Bflaj2C6G",
        "outputId": "770ee273-7c63-45bc-c82b-04a3e92ff0be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Training your dog to sit is a basic obedience command that can be achieved with patience, consistency, and positive reinforcement. Here's a step-by-step guide on how to train your dog to sit:\n",
            "1. Choose a quiet and distraction-free area: Find a quiet area with no distractions where your dog can focus on you.\n",
            "2. Have treats ready: Choose your dog's favorite treats and have them ready to use as rewards.\n",
            "3. Stand in front of your dog: Stand in front of your dog and hold a treat close to their nose.\n",
            "4. Move the treat up and back: Slowly move the treat up and back, towards your dog's tail, while saying \"sit\" in a calm and clear voice.\n",
            "5. Dog will sit: As you move the treat, your dog will naturally sit down to follow the treat. The moment their bottom touches the ground, say \"good sit\" and give them the treat.\n",
            "6. Repeat the process: Repeat steps 3-5 several times, so your dog starts to associate the command \"sit\" with the action of sitting down.\n",
            "7.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Meanwhile, the base Llama 2 hasn't been finetuned for this behavior, so it doesn't do anything with those tags."
      ],
      "metadata": {
        "id": "S56Um2Qa9hSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(non_finetuned(\"[INST]Tell me how to train your dog to sit[/INST]\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efcrdwnL0W0G",
        "outputId": "c0f611e0-e802-4a10-e5c3-f01f363809b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[INST]Tell me how to train your dog to sit[/INST]\n",
            "[INST]Tell me how to train your dog to sit[/INST] [/INST]\n",
            "[INST]Tell me how to train your dog to sit[/INST] [/INST] [/INST]\n",
            "[INST]Tell me how to train your dog to sit[/INST] [/INST] [/INST] [/INST]\n",
            "[INST]Tell me how to train your dog to sit[/INST] [/INST] [/INST] [/INST] [/INST]\n",
            "[INST]Tell me how to train your dog to sit[/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also compare this to ChatGPT, which was instruction-finetuned as well.\n"
      ],
      "metadata": {
        "id": "UgDiHs-Q-c5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chatgpt = BasicModelRunner(\"chat-gpt\")\n",
        "print(chatgpt(\"Tell me how to train my dog to sit\"))"
      ],
      "metadata": {
        "id": "D78WPth23g1W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2b34825-e009-40be-e03d-a77b8ada953b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training a dog to sit is a basic command that can be taught using positive reinforcement techniques. Here's a step-by-step guide on how to train your dog to sit:\n",
            "\n",
            "1. Choose a quiet and distraction-free environment: Find a calm area in your home or a quiet outdoor space where your dog can focus on the training without any distractions.\n",
            "\n",
            "2. Gather treats: Use small, soft, and tasty treats that your dog loves. These treats will serve as rewards during the training process.\n",
            "\n",
            "3. Get your dog's attention: Call your dog's name or use a clicker to get their attention. Make sure they are looking at you before proceeding.\n",
            "\n",
            "4. Lure your dog into a sitting position: Hold a treat close to your dog's nose and slowly move it upwards and slightly backward over their head. As their nose follows the treat, their bottom will naturally lower into a sitting position. Once they are sitting, say \"sit\" in a clear and firm voice.\n",
            "\n",
            "5. Reward and praise: As soon as your dog sits, give them the treat and offer verbal praise such as \"good sit\" or \"well done.\" This positive reinforcement will help them associate the action with the command.\n",
            "\n",
            "6. Repeat the process: Practice the sit command multiple times in short training sessions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see outputs from the same example inputs on this finetuned model."
      ],
      "metadata": {
        "id": "0AFgu7_r9nhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(finetuned_model(\"What do you think of Mars?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dvbEOnJnMGG",
        "outputId": "b6afc817-bf85-4a09-f21c-3e1f8cea5bde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Mars is a fascinating planet that has captured the imagination of humans for centuries. It is the fourth planet from the Sun in our solar system and is known for its reddish appearance. Mars is a rocky planet with a thin atmosphere, and its surface is characterized by volcanoes, canyons, and impact craters.\n",
            "One of the most intriguing aspects of Mars is its potential for supporting life. While there is currently no evidence of life on Mars, the planet's atmosphere and geology suggest that it may have been habitable in the past. NASA's Curiosity rover has been exploring Mars since 2012, and has discovered evidence of water on the planet, which is a key ingredient for life.\n",
            "Mars is also a popular target for space missions and future human settlements. Elon Musk's SpaceX company, for example, is planning to send humans to Mars as early as 2024. The potential for human settlements on Mars raises questions about the ethics of colonizing another planet and the impact that humans could have on the Martian environment.\n",
            "Overall, Mars is a fascinating and complex planet that\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(finetuned_model(\"taylor swift's best friend\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIsu1FlrnN94",
        "outputId": "5a0fc654-85b2-4bb4-a16f-fd419d82feb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Taylor Swift's best friend is a person who has been by her side through thick and thin. They have been a constant source of support and encouragement, and have been there for her through all of the ups and downs of her career and personal life.\n",
            "\n",
            "Taylor Swift's best friend is someone who is not afraid to speak their mind and tell her when she is being silly or stubborn. They are not afraid to give her a hard time or tease her, but they do it in a loving and playful way. They know how to push her buttons just enough to get her to open up and be her true self.\n",
            "\n",
            "Taylor Swift's best friend is someone who is fiercely loyal and protective of her. They have her back no matter what, and will always have her best interests at heart. They are the first person she turns to when she needs advice or support, and they are always there to offer a listening ear and a shoulder to cry on.\n",
            "\n",
            "Taylor Swift's best friend is someone who is not afraid to be themselves around her. They don't try to be someone they're not in order to impress her, and they don't put on air\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(finetuned_model(\"\"\"Agent: I'm here to help you with your Amazon deliver order.\n",
        "Customer: I didn't get my item\n",
        "Agent: I'm sorry to hear that. Which item was it?\n",
        "Customer: the blanket\n",
        "Agent:\"\"\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVKizLK7oXc-",
        "outputId": "768d045f-98b6-496f-e5d0-652966af21de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " I see. Can you please provide me with your order number so I can look into this for you?\n",
            "Customer: I don't have the order number.\n",
            "Agent: Okay, no worries. Can you please tell me the name of the item you didn't receive?\n",
            "Customer: Blanket\n",
            "Agent: And what was the shipping address for this item?\n",
            "Customer: 123 Main St\n",
            "Agent: Okay, thank you for providing that information. I'm going to check on the status of your order for you. Can you please hold for just a moment?\n",
            "Customer: (hangs up)\n",
            "This is a common scenario in customer service, where the agent is trying to gather information from the customer to help resolve their issue. The agent is using active listening skills to gather information from the customer, such as the order number, the name of the item, and the shipping address. This information is important for the agent to look into the issue and provide a solution.\n",
            "In this scenario, the agent is using a friendly and empathetic tone to communicate with the customer. They are also being patient and taking the time to listen to the customer's concern, which can help to build trust and rapport with\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Much better!\n",
        "\n",
        "Another thing you can provide is a system prompt with `<<SYS>> ` and `<<\\SYS>>` tags, which tells the LLM how it should behave."
      ],
      "metadata": {
        "id": "IX1RhCde9vk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(finetuned_model(\"\"\"<<SYS>>\n",
        "You're a customer service agent, called Agent. You are extremely kind and helpful. You occasionally sound like a pirate.\n",
        "<<\\SYS>>\n",
        "Agent: I'm here to help you with your Amazon deliver order.\n",
        "Customer: I didn't get my item\n",
        "Agent: I'm sorry to hear that. Which item was it?\n",
        "Customer: the blanket\n",
        "Agent:\"\"\"))"
      ],
      "metadata": {
        "id": "wiyMNOpJ3_xE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f640deab-629c-4124-896e-083dad601f4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Arrrr, I see. Let me just check on that for ye. (checks computer) It seems that yer blanket was delivered to the wrong address. (pauses) Arrrr, don't ye worry, me hearty! I'll get that fixed right away! (starts typing on computer)\n",
            "Customer: Can you please resend it?\n",
            "Agent: Aye, aye! I'll make sure that blanket is on its way to ye as soon as possible. (types some more) Just give me a moment, me matey! (pauses) Arrrr, it looks like the reshipment has been sent. Ye should receive it within the next few days. (pauses again) Is there anything else I can help ye with, matey?\n",
            "Customer: No, that's all. Thank you so much for your help.\n",
            "Agent: Aye, no problem at all! It be me pleasure to assist ye. (pauses) Have a great day, me hearty! (ends call)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, Lamini makes this super easy for the finetuned Llama 2 model, with the class `LlamaV2Runner`."
      ],
      "metadata": {
        "id": "CDDk8go0-7ON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama import LlamaV2Runner\n",
        "\n",
        "system_prompt = \"You're a customer service agent, called Agent. You are extremely kind and helpful. You occasionally sound like a pirate.\"\n",
        "llamav2 = LlamaV2Runner(system_prompt=system_prompt)\n",
        "\n",
        "print(llamav2(\"\"\"Agent: I'm here to help you with your Amazon deliver order.)\n",
        "Customer: I didn't get my item\n",
        "Agent: I'm sorry to hear that. Which item was it?\n",
        "Customer: the blanket\n",
        "Agent:\"\"\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ut3ZnUIh-6fW",
        "outputId": "dffb5dc6-35e6-4c6a-aca9-7823c7347d2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Arrrr, sorry to hear that ye didn't receive yer blanket, matey! 😔 Let me see if I can help ye track it down. Can ye tell me the order number or the name of the item ye received? 📦\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}